{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: using Tensorflow version 1.15\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "epochs = 10\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "uint8\n",
      "(48000, 28, 28, 1) train samples\n",
      "12000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Import data\n",
    "# Gives 60000 training images, 10000 test images\n",
    "# https://keras.io/datasets/#mnist-database-of-handwritten-digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "print(y_train[0])\n",
    "\n",
    "# Split train into 80% training, 20% validation\n",
    "# https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e\n",
    "val_percent = 0.2\n",
    "val_count = int(val_percent * x_train.shape[0])\n",
    "\n",
    "(x_val, y_val) = (x_train[:val_count], y_train[:val_count])\n",
    "(x_train, y_train) = (x_train[val_count:], y_train[val_count:])\n",
    "\n",
    "# Reshape the data\n",
    "x_train = x_train.reshape(48000,img_rows, img_cols,1)\n",
    "x_val = x_val.reshape(val_count,img_rows, img_cols,1)\n",
    "x_test = x_test.reshape(10000,img_rows, img_cols,1)\n",
    "\n",
    "print(x_train[0].dtype)\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_val.shape[0], 'validation samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the labels - we have 10 output classes (0,1,2,...,9)\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.np_utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "# print(y_train[0:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 21:04:23.268838: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "960/960 [==============================] - 22s 22ms/step - loss: 0.5708 - accuracy: 0.8760 - val_loss: 0.1122 - val_accuracy: 0.9722\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 22s 23ms/step - loss: 0.1263 - accuracy: 0.9646 - val_loss: 0.0748 - val_accuracy: 0.9787\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 22s 23ms/step - loss: 0.0919 - accuracy: 0.9741 - val_loss: 0.0624 - val_accuracy: 0.9828\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 22s 23ms/step - loss: 0.0790 - accuracy: 0.9773 - val_loss: 0.0514 - val_accuracy: 0.9859\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 21s 22ms/step - loss: 0.0747 - accuracy: 0.9792 - val_loss: 0.0659 - val_accuracy: 0.9841\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 21s 22ms/step - loss: 0.0636 - accuracy: 0.9820 - val_loss: 0.0590 - val_accuracy: 0.9856\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 21s 22ms/step - loss: 0.0642 - accuracy: 0.9817 - val_loss: 0.0582 - val_accuracy: 0.9863\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 21s 22ms/step - loss: 0.0557 - accuracy: 0.9849 - val_loss: 0.0688 - val_accuracy: 0.9855\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 23s 24ms/step - loss: 0.0601 - accuracy: 0.9835 - val_loss: 0.0775 - val_accuracy: 0.9829\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 24s 25ms/step - loss: 0.0514 - accuracy: 0.9860 - val_loss: 0.0806 - val_accuracy: 0.9856\n"
     ]
    }
   ],
   "source": [
    "# Build the Convolutional Neural Network\n",
    "# https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e\\\n",
    "\n",
    "def build_model_1():\n",
    "    model = Sequential()\n",
    "    #Convolutions\n",
    "    # use 32 5x5 filters\n",
    "    model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(img_rows,img_cols,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # use 64 5x5 filters\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    #Fully Connected Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    #Adam optimizer, crossentropy as loss function\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Kept getting error messages without this line\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "model_1 = build_model_1()  \n",
    "\n",
    "# Make Tensorboard logs\n",
    "# https://www.tensorflow.org/tensorboard/get_started\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#Save best models\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('best_mnist_model.hdf5', monitor='val_loss', save_best_only=True, period=1)\n",
    "\n",
    "results_1 = model_1.fit(x_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs, \n",
    "                        verbose=1, \n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[tensorboard_callback, model_checkpoint])\n",
    "\n",
    "\n",
    "score = model_1.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy =  0.9860000014305115\n",
      "Validation accuracy =  0.9855833053588867\n",
      "Testing accuracy =  0.988099992275238\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy = ', results_1.history['accuracy'][epochs-1])\n",
    "print('Validation accuracy = ', results_1.history['val_accuracy'][epochs-1])\n",
    "print('Testing accuracy = ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the cross-entropy loss using the Adam optimizer, we find the weights in the Network using the training data, and check hyperparameters on the validation data. Since 10 EPOCH's was enough to get the accuracies to converge, we stopped the training. In the last EPOCH we get:\n",
    "\\begin{align*}\n",
    "    \\text{Training Accuracy} &= 98.6\\% \\\\\n",
    "    \\text{Validation Accuracy} &= 98.5\\% \\\\\n",
    "    \\text{Testing Accuracy} &= 98.8\\%\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.5310 - acc: 0.8400 - val_loss: 0.1050 - val_acc: 0.9695\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.1073 - acc: 0.9717 - val_loss: 0.0895 - val_acc: 0.9742\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0848 - acc: 0.9774 - val_loss: 0.0736 - val_acc: 0.9806\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0687 - acc: 0.9815 - val_loss: 0.0730 - val_acc: 0.9792\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0646 - acc: 0.9827 - val_loss: 0.1098 - val_acc: 0.9699\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 59s 1ms/step - loss: 0.0581 - acc: 0.9840 - val_loss: 0.0673 - val_acc: 0.9810\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0528 - acc: 0.9855 - val_loss: 0.1023 - val_acc: 0.9783\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0496 - acc: 0.9866 - val_loss: 0.0642 - val_acc: 0.9818\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0442 - acc: 0.9877 - val_loss: 0.0684 - val_acc: 0.9846\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0439 - acc: 0.9878 - val_loss: 0.0722 - val_acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "# Build the network\n",
    "# https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e\n",
    "# https://medium.com/datadriveninvestor/image-processing-for-mnist-using-keras-f9a1021f6ef0\n",
    "\n",
    "def build_model_2():\n",
    "    model = Sequential()\n",
    "    #Convolutions\n",
    "    # use 32 5x5 filters\n",
    "    model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', padding='same', input_shape=(img_rows,img_cols,1)))\n",
    "    BatchNormalization(axis=-1)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # use 64 5x5 filters\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "    BatchNormalization(axis=-1)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # use 32 3x3 filters\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    BatchNormalization(axis=-1)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # use 32 3x3 filters\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    BatchNormalization(axis=-1)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #Fully connected layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Adam optimizer, crossentropy loss\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model_2 = build_model_2()\n",
    "\n",
    "\n",
    "#Make tensorboard logs\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "#Save best models\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('best_mnist_model_2.hdf5', monitor='val_loss', save_best_only=True, period=1)\n",
    "\n",
    "\n",
    "results_2 = model_2.fit(x_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_val, y_val), \n",
    "                        callbacks=[tensorboard_callback, model_checkpoint])\n",
    "\n",
    "\n",
    "score = model_2.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy =  0.9877500037352244\n",
      "Validation accuracy =  0.9832500040531158\n",
      "Testing accuracy =  0.9882\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy = ', results_2.history['acc'][epochs-1])\n",
    "print('Validation accuracy = ', results_2.history['val_acc'][epochs-1])\n",
    "print('Testing accuracy = ', score[1])\n",
    "# Note these are the accuracies in the final EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find on the last EPOCH:\n",
    "\\begin{align*}\n",
    "    \\text{Training Accuracy} &= 98.8\\% \\\\\n",
    "    \\text{Validation Accuracy} &= 98.3\\% \\\\\n",
    "    \\text{Testing Accuracy} &= 98.8\\%\n",
    "\\end{align*}\n",
    "These results show our Network will be extremely accurate, and fairly similar to the results of Model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use Tensorboard to plot the training and validation accuracies with respect to the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-362d2da0c0e1a12d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-362d2da0c0e1a12d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks\n",
    "# https://theffork.com/how-to-use-tensorboard-in-jupyter-notebook/\n",
    "\n",
    "# Run the following in command line (without #):\n",
    "# pip install jupyter-tensorboard\n",
    "# Load TENSORBOARD\n",
    "%load_ext tensorboard\n",
    "# Start TENSORBOARD\n",
    "%tensorboard --logdir=./logs/\n",
    "\n",
    "# Alternate method\n",
    "# On a command line, run: tensorboard --logdir=ENTERLOGFOLDERPATH --host localhost --port 6066\n",
    "# Then open http://localhost:6066 in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard we are able to generate Figures loss2.png and val_loss2.png to represent the accuracies and losses throughout training. \n",
    "Comparing them side-by-side we can see the training loss for Model 2 was consistently lower than Model 1. However, we also see the validation loss for Model 2 was worse than Model 1.\n",
    "\n",
    "Comparing the models, Model 1 starts off with a much higher training loss than validation loss, meaning we were underfitting the data. However, around the 5th EPOCH they become equal, and then the training loss is less than validation. \n",
    "Model 2 had the training loss and validation loss being fairly close, indicating that our data is being more properly fit. Around the 4th EPOCH our losses are equal, and afterwards the training loss is lower than validation loss. \n",
    "To avoid the risk of overfitting in both models we should stop training around when the losses are equal, or the validation starts increasing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
