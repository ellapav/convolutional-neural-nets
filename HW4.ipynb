{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# note: using Tensorflow version 1.15\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "uint8\n",
      "(48000, 28, 28, 1) train samples\n",
      "12000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Import data\n",
    "# Gives 60000 training images, 10000 test images\n",
    "# https://keras.io/datasets/#mnist-database-of-handwritten-digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "print(y_train[0])\n",
    "\n",
    "# Split train into 80% training, 20% validation\n",
    "# https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e\n",
    "val_percent = 0.2\n",
    "val_count = int(val_percent * x_train.shape[0])\n",
    "\n",
    "(x_val, y_val) = (x_train[:val_count], y_train[:val_count])\n",
    "(x_train, y_train) = (x_train[val_count:], y_train[val_count:])\n",
    "\n",
    "# Reshape the data\n",
    "x_train = x_train.reshape(48000,img_rows, img_cols,1)\n",
    "x_val = x_val.reshape(val_count,img_rows, img_cols,1)\n",
    "x_test = x_test.reshape(10000,img_rows, img_cols,1)\n",
    "\n",
    "print(x_train[0].dtype)\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_val.shape[0], 'validation samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the labels - we have 10 output classes (0,1,2,...,9)\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 23s 479us/step - loss: 1.5986 - acc: 0.8455 - val_loss: 0.0785 - val_acc: 0.9767\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 22s 465us/step - loss: 0.1073 - acc: 0.9698 - val_loss: 0.0813 - val_acc: 0.9781\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 24s 503us/step - loss: 0.0836 - acc: 0.9762 - val_loss: 0.0640 - val_acc: 0.9821\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 23s 478us/step - loss: 0.0703 - acc: 0.9800 - val_loss: 0.0542 - val_acc: 0.9850\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 22s 465us/step - loss: 0.0627 - acc: 0.9811 - val_loss: 0.0640 - val_acc: 0.9842\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 22s 454us/step - loss: 0.0596 - acc: 0.9840 - val_loss: 0.0724 - val_acc: 0.9831\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 23s 484us/step - loss: 0.0551 - acc: 0.9845 - val_loss: 0.0776 - val_acc: 0.9828\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 22s 462us/step - loss: 0.0535 - acc: 0.9850 - val_loss: 0.0743 - val_acc: 0.9853\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 22s 451us/step - loss: 0.0518 - acc: 0.9866 - val_loss: 0.0666 - val_acc: 0.9849\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 23s 471us/step - loss: 0.0486 - acc: 0.9872 - val_loss: 0.0624 - val_acc: 0.9863\n"
     ]
    }
   ],
   "source": [
    "# Build the Convolutional Neural Network\n",
    "# https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e\\\n",
    "\n",
    "def build_model_1():\n",
    "    model = Sequential()\n",
    "    #Convolutions\n",
    "    # use 32 5x5 filters\n",
    "    model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(img_rows,img_cols,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # use 64 5x5 filters\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    #Fully Connected Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    #Adam optimizer, crossentropy as loss function\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Kept getting error messages without this line\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "model_1 = build_model_1()  \n",
    "\n",
    "# Make Tensorboard logs\n",
    "# https://www.tensorflow.org/tensorboard/get_started\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#Save best models\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('best_mnist_model.hdf5', monitor='val_loss', save_best_only=True, period=1)\n",
    "\n",
    "results_1 = model_1.fit(x_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs, \n",
    "                        verbose=1, \n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[tensorboard_callback, model_checkpoint])\n",
    "\n",
    "\n",
    "score = model_1.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy =  0.9871875047062834\n",
      "Validation accuracy =  0.9863333384195964\n",
      "Testing accuracy =  0.9879\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy = ', results_1.history['acc'][epochs-1])\n",
    "print('Validation accuracy = ', results_1.history['val_acc'][epochs-1])\n",
    "print('Testing accuracy = ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.5310 - acc: 0.8400 - val_loss: 0.1050 - val_acc: 0.9695\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.1073 - acc: 0.9717 - val_loss: 0.0895 - val_acc: 0.9742\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0848 - acc: 0.9774 - val_loss: 0.0736 - val_acc: 0.9806\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0687 - acc: 0.9815 - val_loss: 0.0730 - val_acc: 0.9792\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0646 - acc: 0.9827 - val_loss: 0.1098 - val_acc: 0.9699\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 59s 1ms/step - loss: 0.0581 - acc: 0.9840 - val_loss: 0.0673 - val_acc: 0.9810\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0528 - acc: 0.9855 - val_loss: 0.1023 - val_acc: 0.9783\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0496 - acc: 0.9866 - val_loss: 0.0642 - val_acc: 0.9818\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0442 - acc: 0.9877 - val_loss: 0.0684 - val_acc: 0.9846\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0439 - acc: 0.9878 - val_loss: 0.0722 - val_acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "# Build the network\n",
    "# https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e\n",
    "# https://medium.com/datadriveninvestor/image-processing-for-mnist-using-keras-f9a1021f6ef0\n",
    "\n",
    "def build_model_2():\n",
    "    model = Sequential()\n",
    "    #Convolutions\n",
    "    # use 32 5x5 filters\n",
    "    model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', padding='same', input_shape=(img_rows,img_cols,1)))\n",
    "    BatchNormalization(axis=-1)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # use 64 5x5 filters\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "    BatchNormalization(axis=-1)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # use 32 3x3 filters\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    BatchNormalization(axis=-1)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # use 32 3x3 filters\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    BatchNormalization(axis=-1)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #Fully connected layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Adam optimizer, crossentropy loss\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model_2 = build_model_2()\n",
    "\n",
    "\n",
    "#Make tensorboard logs\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "#Save best models\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('best_mnist_model_2.hdf5', monitor='val_loss', save_best_only=True, period=1)\n",
    "\n",
    "\n",
    "results_2 = model_2.fit(x_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_val, y_val), \n",
    "                        callbacks=[tensorboard_callback, model_checkpoint])\n",
    "\n",
    "\n",
    "score = model_2.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy =  0.9877500037352244\n",
      "Validation accuracy =  0.9832500040531158\n",
      "Testing accuracy =  0.9882\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy = ', results_2.history['acc'][epochs-1])\n",
    "print('Validation accuracy = ', results_2.history['val_acc'][epochs-1])\n",
    "print('Testing accuracy = ', score[1])\n",
    "# Note these are the accuracies in the final EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1495), started 1:10:29 ago. (Use '!kill 1495' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9f62b7c33ceb9d0f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9f62b7c33ceb9d0f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks\n",
    "# https://theffork.com/how-to-use-tensorboard-in-jupyter-notebook/\n",
    "\n",
    "# Run the following in command line (without #):\n",
    "# pip install jupyter-tensorboard\n",
    "# Load TENSORBOARD\n",
    "%load_ext tensorboard\n",
    "# Start TENSORBOARD\n",
    "%tensorboard --logdir=./logs/\n",
    "\n",
    "# Alternate method\n",
    "# On a command line, run: tensorboard --logdir=ENTERLOGFOLDERPATH --host localhost --port 6066\n",
    "# Then open http://localhost:6066 in your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
